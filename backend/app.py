from flask import Flask, request, jsonify
from flask_cors import CORS
from src.helper import download_hugging_face_embeddings
from langchain_pinecone import PineconeVectorStore
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
from langchain_community.llms import Ollama
from src.prompt import *
import os

# --- 1. CONFIGURATION INITIALE (Se lance une seule fois au d√©marrage) ---
app = Flask(__name__)
CORS(app) # Autorise ton Frontend React √† parler √† ce serveur

load_dotenv()

# V√©rification de la cl√© API
PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')
if not PINECONE_API_KEY:
    print("Erreur : PINECONE_API_KEY non trouv√©e dans le fichier .env")
os.environ["PINECONE_API_KEY"] = PINECONE_API_KEY

print("Chargement des Embeddings HuggingFace...")
embeddings = download_hugging_face_embeddings()

index_name = "ensa-chatbot" 

# Connexion √† Pinecone
print("Connexion √† l'index Pinecone...")
docsearch = PineconeVectorStore.from_existing_index(
    index_name=index_name,
    embedding=embeddings
)

retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k":3})

# Initialisation du LLM Local (gemma:2b)
print("Initialisation du mod√®le Ollama (gemma:2b)...")
chatModel = Ollama(model="gemma:2b")

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(chatModel, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

print("‚úÖ Serveur pr√™t ! En attente de requ√™tes...")


# --- 2. D√âFINITION DE LA ROUTE API (Ce que ton React appelle) ---
@app.route('/chat', methods=['POST'])
def chat():
    try:
        # R√©cup√©rer la question envoy√©e par le React
        data = request.json
        user_query = data.get('query')
        
        if not user_query:
            return jsonify({"error": "Aucune question fournie"}), 400

        print(f"üì© Question re√ßue : {user_query}")

        # Lancer la cha√Æne RAG (Recherche + G√©n√©ration)
        response = rag_chain.invoke({"input": user_query})
        
        # R√©cup√©rer la r√©ponse texte
        answer_text = response["answer"]
        
        # (Optionnel) R√©cup√©rer les sources utilis√©es pour r√©pondre
        # sources = [doc.metadata.get('source', 'Inconnu') for doc in response.get("context", [])]

        print("üì§ R√©ponse envoy√©e.")
        
        # Renvoyer le JSON au React
        return jsonify({
            "answer": answer_text,
            # "sources": sources # Tu pourras d√©commenter √ßa plus tard si tu veux afficher les sources
        })

    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        return jsonify({"answer": "D√©sol√©, une erreur technique est survenue sur le serveur."}), 500


# --- 3. LANCEMENT DU SERVEUR ---
if __name__ == '__main__':
    # host='0.0.0.0' permet l'acc√®s depuis d'autres appareils (mobile) sur le m√™me wifi
    app.run(host='0.0.0.0', port=5000, debug=True)